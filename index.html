<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="Gradient-based Analysis of NLP Models is Manipulable">
    <meta name="author" content="">
    <link rel="icon" href="/docs/4.0/assets/img/favicons/favicon.ico">

    <title>Gradient-based Analysis of NLP Models is Manipulable</title>


    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css"
        integrity="sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2" crossorigin="anonymous">
    <!-- <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.2/css/bootstrap.min.css"
        integrity="sha384-Smlep5jCw/wG7hdkwQ/Z5nLIefveQRIY9nfy6xoR1uRYBtpZgI6339F5dgvm/e9B" crossorigin="anonymous">
    <link href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet"> -->


    <!-- Custom styles for this template -->
    <!-- <script src="https://distill.pub/template.v2.js"></script> -->

</head>

<body>

    <!-- Begin page content -->
    <main role="main" class="container">
        <h1 class="mt-5">Gradient-based Analysis of NLP Models is Manipulable</h1>
        <!-- <h4>Authors</h4>
            <h4>Affiliations</h4> -->
        <p>
            <a class="name" href="https://isthatyou.github.io">Junlin Wang</a>,
            <a class="name" href="https://jens321.github.io">Jens Tuyls</a>,
            <a class="name" href="https://www.ericswallace.com">Eric Wallace</a>,
            <a class="name" href="http://sameersingh.org">Sameer Singh</a>
        </p>
        <!-- <p class="author">
                <a class="affiliation" href="https://www.ics.uci.edu">UC Irvine</a>
                <a class="affiliation" href="https://princeton-nlp.github.io">Princeton University</a>
                <a class="affiliation" href="http://nlp.cs.berkeley.edu">UC Berkeley</a>
                <a class="affiliation" href="https://www.ics.uci.edu">UC Irvine</a>
            </p> -->
        <div>
            <!-- <h3>Published</h3> -->
            <p>Oct. 5, 2020</p>
        </div>

        <div>
            <div class="wrapper">
                <button type="button" class="btn btn-info">Paper</button>
                <button type="button" class="btn btn-secondary">Code</button>
            </div>
            <h2>Background</h2>
            <p>
                Many modern interpretation techniques rely on gradients as they are often seen as faithful
                representations
                of a
                model: they depend on all of the model parameters, are completely faithful when the model is linear, and
                closely
                approximate the model nearby an input.

                We show that even these “faithful” interpretations can be manipulated to be completely unreliable
                indicators
                of
                a
                model’s actual reasoning. We apply our technique to text classification, NLI, and Question Answering.
            </p>
            <h2>Our Method</h2>
            <p>
                Given an off-the-shelf language model (e.g., BERT), we merge this model with a model we call “Facade”.
                The
                gradients
                of the Facade are trained to be adversarial (e.g., focus on only the first token in the sentence), while
                its
                outputs
                are kept uniform. This way, the resulting merged model (original + Facade) has the same behavior as the
                original
                model, but its gradients will be dominated by the Facade! The figure below summarizes our attack.
            <figure>
                <img class="figure-img img-fluid rounded mx-auto d-block" style="width:85%" src="images/overview.png">
            </figure>
            <p>
                To guarantee that the behavior of the model stays the same after merging, we carefully intertwine the
                weights of
                the
                original model and the Facade, and only sum at the output layer (see paper for details).
            </p>
            <h2>Qualitative Results</h2>
            <p>
                Our technique fools many popular gradient-based interpretation techniques such as Gradient, SmoothGrad,
                and
                Integrated Gradient, though the last one is less affected. Our method also causes input reduction to
                reduce
                to
                unimportant tokens, and hotflip to require more perturbations to change the model prediction.
            </p>
            <figure>
                <img class="figure-img img-fluid mx-auto d-block" src="images/figure1.png">
            </figure>
            <p>
                For the example above, we take a BERT-based sentiment classifier and merge its weights with a Facade.
                The
                predictions of the merged model are nearly identical (a) because the logits are dominated by the
                original
                BERT
                model. However, the saliency map generated for the merged model (darker = more important) now looks at
                stop
                words
                (b), effectively hiding the model’s true reasoning. Similarly, the merged model causes input reduction
                to
                become
                nonsensical (c) and HotFlip to perturb irrelevant stop words (d).
            </p>
            <!-- <h2>Summary</h2>
            <p>
                In this paper, we showed that gradient-based analysis is easily manipulable. To accomplish this, we
                created
                a
                FACADE
                classifier with misleading gradients that can be merged with any given model of interest. The merged
                model
                has
                similar predictions as the original model, but with gradients dominated by the FACADE model. We hope to
                encourage
                the development of robust analysis techniques, as well as methods to detect adversarial model
                modifications.
            </p> -->

        </div>
    </main>


</body>

</html>