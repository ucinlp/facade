<!--
  Copyright 2018 The Distill Template Authors
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
       http://www.apache.org/licenses/LICENSE-2.0
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->

<!doctype html>
<meta charset="utf-8">
<script src="https://distill.pub/template.v2.js"></script>
<d-title>
    <h1>Gradient-based Analysis of NLP Models is Manipulable</h1>
</d-title>
<d-byline>
    <div class="byline grid">
        <div class="authors-affiliations grid">
            <h3>Authors</h3>
            <h3>Affiliations</h3>
            <p class="author">
                <a class="name" href="https://isthatyou.github.io">Junlin Wang</a>
            </p>
            <p class="affiliation">
                <a class="affiliation" href="https://www.ics.uci.edu">UC Irvine</a>
            </p>
            <p class="author">
                <a class="name" href="https://jens321.github.io">Jens Tuyls</a>
            </p>
            <p class="affiliation">
                <a class="affiliation" href="https://princeton-nlp.github.io">Princeton University</a>
            </p>
            <p class="author">
                <a class="name" href="https://www.ericswallace.com">Eric Wallace</a>
            </p>
            <p class="affiliation">
                <a class="affiliation" href="http://nlp.cs.berkeley.edu">UC Berkeley</a>
            </p>
            <p class="author">
                <a class="name" href="http://sameersingh.org">Sameer Singh</a>
            </p>
            <p class="affiliation">
                <a class="affiliation" href="https://www.ics.uci.edu">UC Irvine</a>
            </p>
        </div>
        <div>
            <h3>Published</h3>
            <p>Oct. 5, 2020</p>
            <h3>Paper</h3>
            <a>Coming Soon</a>
            <h3>Code</h3>
            <a>Coming Soon</a>
        </div>
    </div>
</d-byline>


<d-article>
    <h2>Background</h2>
    <p>
        Many modern interpretation techniques rely on gradients as they are often seen as faithful representations
        of a
        model: they depend on all of the model parameters, are completely faithful when the model is linear, and
        closely
        approximate the model nearby an input.

        We show that even these “faithful” interpretations can be manipulated to be completely unreliable indicators
        of
        a
        model’s actual reasoning. We apply our technique to text classification, NLI, and Question Answering.
    </p>
    <h2>Our Method</h2>
    <p>
        Given an off-the-shelf language model (e.g., BERT), we merge this model with a model we call “Facade”. The
        gradients
        of the Facade are trained to be adversarial (e.g., focus on only the first token in the sentence), while its
        outputs
        are kept uniform. This way, the resulting merged model (original + Facade) has the same behavior as the
        original
        model, but its gradients will be dominated by the Facade! The figure below summarizes our attack.
    <figure class="l-page">
        <d-figure><img src="images/overview.png"></d-figure>
    </figure>
    <p>
        To guarantee that the behavior of the model stays the same after merging, we carefully intertwine the
        weights of
        the
        original model and the Facade, and only sum at the output layer (see paper for details).
    </p>
    <h2>Qualitative Results</h2>
    <p>
        Our technique fools many popular gradient-based interpretation techniques such as Gradient, SmoothGrad, and
        Integrated Gradient, though the last one is less affected. Our method also causes input reduction to reduce
        to
        unimportant tokens, and hotflip to require more perturbations to change the model prediction.
    </p>
    <figure class="l-body">
        <d-figure><img src="images/figure1.png"></d-figure>
    </figure>
    <p>
        For the example above, we take a BERT-based sentiment classifier and merge its weights with a Facade. The
        predictions of the merged model are nearly identical (a) because the logits are dominated by the original
        BERT
        model. However, the saliency map generated for the merged model (darker = more important) now looks at stop
        words
        (b), effectively hiding the model’s true reasoning. Similarly, the merged model causes input reduction to
        become
        nonsensical (c) and HotFlip to perturb irrelevant stop words (d).
    </p>
    <h2>Summary</h2>
    <p>
        In this paper, we showed that gradient-based analysis is easily manipulable. To accomplish this, we created
        a
        FACADE
        classifier with misleading gradients that can be merged with any given model of interest. The merged model
        has
        similar predictions as the original model, but with gradients dominated by the FACADE model. We hope to
        encourage
        the development of robust analysis techniques, as well as methods to detect adversarial model modifications.
    </p>

    </dt-article>

    <dt-appendix>
    </dt-appendix>